# -*- coding: utf-8 -*-
"""CS6220_Spotify_Music_Recommendation_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/lihaohl0307/Spotify-Music-Recommendation/blob/main/CS6220_Spotify_Music_Recommendation_System.ipynb

# **Final Project: Spotify Music Recommendation System**

### Team Members:
### placeholder

### Package Installation
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import spotipy
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import euclidean_distances
from scipy.spatial.distance import cdist
from spotipy.oauth2 import SpotifyClientCredentials

"""### Data Import"""

data = pd.read_csv("./spotify-dataset/data/data.csv")
songs_data = pd.read_csv('./spotify-dataset/data/data.csv')
genre_data = pd.read_csv('./spotify-dataset/data/data_by_genres.csv')
print(data.shape)
print(data.head(5))

song_cluster_pipeline = Pipeline([('scaler', StandardScaler()),
                                  ('kmeans', KMeans(n_clusters=20,
                                   verbose=False))
                                 ], verbose=False)

X = data.select_dtypes(np.number)
number_cols = list(X.columns)
song_cluster_pipeline.fit(X)
song_cluster_labels = song_cluster_pipeline.predict(X)
data['cluster_label'] = song_cluster_labels

"""### Connect to Spotify API"""

client_id = "7d79fe83da984659b5581054295c34c9"
client_secret = "432302ce5805441dac55670e8a6144bd"
client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

# API testing
name = ["Michael Jackson","pitbull","Christina","Elvis Presley"]
result = sp.search(name)
result['tracks']['items'][1]['artists']

"""## K-means clustering on songs"""

# Reload data in seperate data frame
songs_df = pd.read_csv('/content/drive/My Drive/spotify-dataset/data/data.csv')

selected_features = ['acousticness', 'danceability', 'energy', 'instrumentalness',
                     'liveness', 'loudness', 'speechiness', 'valence', 'tempo',
                     'duration_ms', 'key', 'mode']

"""### PCA for Dimensionality Reduction (retain 95% variance)"""

# Standardize the Features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(songs_df[selected_features])

pca = PCA(n_components=0.95, random_state=42) # 95%
pca_features = pca.fit_transform(scaled_features)

explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

print(f"Number of components selected: {pca.n_components_}")
print(f"Explained variance ratio: {pca.explained_variance_ratio_}")

# Calculate Cumulative Variance for Full Components
pca_full = PCA(random_state=42)
pca_full.fit(scaled_features)
explained_variance_full = pca_full.explained_variance_ratio_
cumulative_variance_full = np.cumsum(explained_variance_full)

# Plot total explained variance
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(cumulative_variance_full) + 1), cumulative_variance_full, color='black', label="Cumulative Explained Variance")
plt.axhline(y=0.95, color='red', linestyle='--', label="95% Variance Threshold")
plt.axvline(x=pca.n_components_, color='blue', linestyle='--', label=f"{pca.n_components_} Components")

plt.xlabel('Number of Components')
plt.ylabel('Total Explained Variance')
plt.title('Explained Variance vs Number of Components')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# plt.figure(figsize=(10, 7))
# plt.plot(cumulative_variance, color='k', lw=2)
# plt.xlabel('Number of components')
# plt.ylabel('Total explained variance')
# plt.xlim(0, len(selected_features))
# plt.yticks(np.arange(0, 1.1, 0.1))
# plt.axvline(pca.n_components_, c='b', label=f'{pca.n_components_} components')
# plt.axhline(0.95, c='r', label='95% variance')
# plt.legend()
# plt.show()

# Principal Component Loadings - detailed weighted features for each PC
loadings = pd.DataFrame(
    pca.components_.T,
    columns=[f'Principal Component {i+1}' for i in range(pca.n_components_)],
    index=selected_features
)

print("Feature Contributions to Principal Components:")
print(loadings)

# PC and explained variance display
components_df = pd.DataFrame({
    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance))],
    'Explained Variance': explained_variance,
    'Cumulative Variance': cumulative_variance
})

print("\nExplained Variance for Principal Components:")
print(components_df)

"""### Elbow Method to determine optimal k for clustering"""

inertias = []
k_values = range(5, 30)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_features)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(k_values, inertias, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()

"""### Apply K-means Clustering"""

optimal_k = 8
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans.fit(pca_features)

# Assign clusters to the original DataFrame
songs_df['cluster'] = kmeans.labels_

# Function to retrieve song data from Spotify API
def find_song_from_spotify(name, year):
    song_data = defaultdict()
    results = sp.search(q=f'track:{name} year:{year}', limit=1)
    if results['tracks']['items'] == []:
        return None

    results = results['tracks']['items'][0]
    track_id = results['id']
    audio_features = sp.audio_features(track_id)[0]

    # Basic metadata
    song_data['name'] = results['name']
    song_data['artists'] = ', '.join([artist['name'] for artist in results['artists']])
    song_data['year'] = year
    song_data['duration_ms'] = results['duration_ms']
    song_data['explicit'] = int(results['explicit'])

    # Audio features
    for key, value in audio_features.items():
        if key in selected_features:
            song_data[key] = value

    return song_data

def recommend_songs_kmeans_with_api(song_names, songs_df, pca_model, scaler, kmeans_model, n_recommendations=10):
    # Ensure 'cluster' column exists
    if 'cluster' not in songs_df.columns:
        raise KeyError("'cluster' column not found in songs_df. Ensure k-means clustering is performed.")

    clusters = []
    input_song_names = []
    new_songs = []  # Store newly added songs for dynamic clustering

    for song_name, year in song_names:
        matched_songs = songs_df[songs_df['name'].str.lower() == song_name.lower()]
        if matched_songs.empty:
            print(f"Song '{song_name}' not found in the dataset. Fetching from Spotify...")
            # Attempt to fetch song from Spotify API
            spotify_song_data = find_song_from_spotify(song_name, year)
            if spotify_song_data:
                new_songs.append(spotify_song_data)
                print(f"Added '{spotify_song_data['name']}' from Spotify.")
            else:
                print(f"Could not find '{song_name}' on Spotify.")
            continue

        input_song_names.append(song_name.lower())
        song_clusters = matched_songs['cluster'].unique()
        clusters.extend(song_clusters)

    # Dynamically cluster new songs (if any)
    if new_songs:
        new_songs_df = pd.DataFrame(new_songs)
        scaled_new_features = scaler.transform(new_songs_df[selected_features])
        pca_new_features = pca_model.transform(scaled_new_features)
        new_clusters = kmeans_model.predict(pca_new_features)
        new_songs_df['cluster'] = new_clusters
        songs_df = pd.concat([songs_df, new_songs_df], ignore_index=True)
        clusters.extend(new_clusters)

    if not clusters:
        print("No matching songs found in the dataset or Spotify.")
        return []

    # Unique clusters
    clusters = list(set(clusters))

    # Songs in the same cluster(s)
    recommended_songs = songs_df[songs_df['cluster'].isin(clusters)]

    # Exclude input songs
    recommended_songs = recommended_songs[~recommended_songs['name'].str.lower().isin(input_song_names)]

    # Randomly select recommendations
    recommended_songs = recommended_songs.sample(n=n_recommendations, random_state=42)
    return recommended_songs[['name', 'year', 'artists', 'cluster']]

# Test input
input_songs = [{'name': 'Come As You Are', 'year':1991},
                {'name': 'Smells Like Teen Spirit', 'year': 1991},
                {'name': 'Lithium', 'year': 1992},
                {'name': 'All Apologies', 'year': 1993},
                {'name': 'Stay Away', 'year': 1993}]

recommendations = recommend_songs_kmeans_with_api(
    input_songs,
    songs_df,
    pca_model=pca,
    scaler=scaler,
    kmeans_model=kmeans,
    n_recommendations=10
)

# Display recommendations
print(recommendations)

"""## Enhanced K-Means Model with Weighted Genre"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from scipy.spatial.distance import cdist
from collections import defaultdict

# Define feature columns
number_cols = ['acousticness', 'danceability', 'energy', 'instrumentalness',
               'liveness', 'loudness', 'speechiness', 'valence', 'tempo',
               'duration_ms', 'key', 'mode']

def classify_songs_by_genre(songs_df, genre_df, feature_columns):
    """
    Assign each song in the songs_df to the closest genre based on shared features in genre_df.
    """
    # Ensure all features are numeric and explicitly cast to float64
    songs_df[feature_columns] = songs_df[feature_columns].apply(pd.to_numeric, errors='coerce').fillna(songs_df[feature_columns].mean())
    genre_df[feature_columns] = genre_df[feature_columns].apply(pd.to_numeric, errors='coerce').fillna(genre_df[feature_columns].mean())

    # Extract genre features and explicitly cast to float64
    genre_features = genre_df[feature_columns].values.astype(np.float64)
    classified_genres = []

    for _, song in songs_df.iterrows():
        # Extract features for the current song and explicitly cast to float64
        song_features = song[feature_columns].values.reshape(1, -1).astype(np.float64)

        # Compute distances between the song and all genres
        distances = cdist(song_features, genre_features, metric='euclidean')

        # Assign the closest genre
        closest_genre_index = distances.argmin()
        closest_genre = genre_df.iloc[closest_genre_index]['genres']
        classified_genres.append(closest_genre)

    # Add the classified genres to the songs DataFrame
    songs_df['classified_genre'] = classified_genres
    return songs_df

# Classify songs into different genres
songs_with_genre_df = classify_songs_by_genre(songs_data, genre_data, number_cols)

songs_with_genre_df.head()

from sklearn.preprocessing import StandardScaler, LabelEncoder

# Encode the 'classified_genre' column into numeric values
label_encoder = LabelEncoder()
songs_with_genre_df['classified_genre_encoded'] = label_encoder.fit_transform(songs_with_genre_df['classified_genre'])

# Update the feature list
selected_features_with_genre = ['acousticness', 'danceability', 'energy', 'instrumentalness',
                                'liveness', 'loudness', 'speechiness', 'valence', 'tempo',
                                'duration_ms', 'key', 'mode', 'classified_genre_encoded'] # use encoded classfied_genre(convert string to float)

# Standardize the Features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(songs_with_genre_df[selected_features_with_genre])

# Apply PCA
pca_genre = PCA(n_components=0.95, random_state=42)  # 95% variance threshold
pca_features_with_genre = pca_genre.fit_transform(scaled_features)

# Calculate Explained Variance
explained_variance_genre_enhanced = pca_genre.explained_variance_ratio_
cumulative_variance_genre_enhanced = np.cumsum(explained_variance_genre_enhanced)

print(f"Number of components selected: {pca_genre.n_components_}")
print(f"Explained variance ratio: {pca_genre.explained_variance_ratio_}")

# apply k-means clustering
optimal_k = 8
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans.fit(pca_features_with_genre)

# Assign clusters to the original DataFrame
songs_with_genre_df['cluster'] = kmeans.labels_

# Test input
input_songs = [{'name': 'Come As You Are', 'year':1991},
                {'name': 'Smells Like Teen Spirit', 'year': 1991},
                {'name': 'Lithium', 'year': 1992},
                {'name': 'All Apologies', 'year': 1993},
                {'name': 'Stay Away', 'year': 1993}]

recommendations = recommend_songs_kmeans_with_api(
    input_songs,
    songs_with_genre_df,
    pca_model=pca_genre,
    scaler=scaler,
    kmeans_model=kmeans,
    n_recommendations=10
)

# Display recommendations
print(recommendations)

"""## Weighted Vector Model with Cosine Similarity - Not yet complete"""

def add_genre_vector(songs_df):
    """
    Add one-hot encoded genre vectors to the songs dataset.
    """
    # One-hot encode the genres
    genre_encoder = OneHotEncoder(sparse_output=False)
    genre_encoded = genre_encoder.fit_transform(songs_df[['classified_genre']])
    songs_df['genre_vector'] = list(genre_encoded)
    return songs_df, genre_encoder

# Add genre vectors
songs_df, genre_encoder = add_genre_vector(songs_df)

# Define feature weights
feature_weights = {
    'valence': 1.0,
    'year': 0.5,
    'acousticness': 1.0,
    'danceability': 1.5,
    'duration_ms': 0.5,
    'energy': 2.0,
    'explicit': 0.1,
    'instrumentalness': 1.0,
    'key': 0.2,
    'liveness': 1.0,
    'loudness': 1.5,
    'mode': 0.5,
    'popularity': 1.0,
    'speechiness': 1.0,
    'tempo': 1.5,
    'genre': 1.0  # Weight for genre vector
}

def get_song_vector_with_weights(song, spotify_data, feature_weights):
    """
    Retrieve the combined song vector (features + genre) with applied weights.
    If the song is not found in the dataset, fetch it from Spotify API.
    """
    try:
        # Try to retrieve the song from the dataset
        song_data = spotify_data[(spotify_data['name'].str.lower() == song['name'].lower()) &
                                 (spotify_data['year'] == song['year'])].iloc[0]
    except IndexError:
        # Song not found in dataset, fetch from Spotify API
        print(f"Fetching '{song['name']}' from Spotify API...")
        song_data = find_song_from_spotify(song['name'], song['year'])
        if song_data is None:
            print(f"Could not find '{song['name']}' on Spotify.")
            return None

    # Combine numerical features and genre vector
    feature_vector = song_data[number_cols].values if isinstance(song_data, pd.Series) else song_data[number_cols].to_numpy()
    genre_vector = np.zeros(len(spotify_data['genre_vector'].iloc[0]))  # Default no genre vector if fetched from API
    if 'genre_vector' in song_data and isinstance(song_data['genre_vector'], np.ndarray):
        genre_vector = np.array(song_data['genre_vector'])

    combined_vector = np.concatenate([feature_vector, genre_vector])

    # Apply weights to the vector
    weighted_features = np.array([feature_vector[i] * feature_weights[col] for i, col in enumerate(number_cols)])
    weighted_genre = genre_vector * feature_weights['genre']
    return np.concatenate([weighted_features, weighted_genre])

def get_weighted_average_vector(song_list, spotify_data, feature_weights):
    """
    Compute a weighted average vector for the input songs, including genre influence.
    """
    vectors = []
    weights = []

    for song in song_list:
        try:
            # Try to retrieve the combined song vector
            song_vector = get_song_vector_with_weights(song, spotify_data, feature_weights)
            if song_vector is not None:
                vectors.append(song_vector)
                weights.append(1.0)  # Adjust weights dynamically if needed
        except Exception as e:
            print(f"Warning: Could not process '{song['name']}' due to {e}. Skipping...")
            continue

    if len(vectors) == 0:
        raise ValueError("No valid song vectors found.")

    vectors = np.vstack(vectors)
    weights = np.array(weights).reshape(-1, 1)
    return np.average(vectors, axis=0, weights=weights.flatten())

def recommend_songs_with_genre(song_list, spotify_data, n_songs=10):
    """
    Recommend songs based on weighted vectors and genre influence.
    """
    metadata_cols = ['name', 'year', 'artists', 'classified_genre']

    # Step 1: Compute the weighted average vector for the input songs
    song_center = get_weighted_average_vector(song_list, spotify_data, feature_weights)

    # Step 2: Scale only the numerical features
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(spotify_data[number_cols])  # Scale numerical features
    scaled_song_center = scaler.transform([song_center[:len(number_cols)]])  # Scale only the numerical part of the input vector

    # Step 3: Combine scaled numerical features with genre vectors
    genre_vectors = np.array(list(spotify_data['genre_vector']))
    combined_data = np.hstack([scaled_data, genre_vectors])  # Combine scaled numerical and genre vectors
    combined_song_center = np.hstack([scaled_song_center[0], song_center[len(number_cols):]])  # Combine input features

    # Step 4: Compute distances and find nearest songs
    distances = cdist([combined_song_center], combined_data, 'cosine')
    index = list(np.argsort(distances)[:, :n_songs][0])

    # Exclude input songs from recommendations
    rec_songs = spotify_data.iloc[index]
    rec_songs = rec_songs[~rec_songs['name'].isin([song['name'] for song in song_list])]
    return rec_songs[metadata_cols].to_dict(orient='records')

# Test input songs
input_songs = [{'name': 'Come As You Are', 'year':1991},
                {'name': 'Smells Like Teen Spirit', 'year': 1991},
                {'name': 'Lithium', 'year': 1992},
                {'name': 'All Apologies', 'year': 1993},
                {'name': 'Stay Away', 'year': 1993}
               ]

# Get recommendations
recommendations = recommend_songs_with_genre(input_songs, songs_df, n_songs=10)

# Convert recommendations to a DataFrame
recommendations_df = pd.DataFrame(recommendations)

# Display the DataFrame
print(recommendations_df)

